{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "lb=LabelBinarizer()\n",
    "\n",
    "# Load the training and test data\n",
    "train_df = pd.read_csv('archive/sign_mnist_train.csv')\n",
    "test_df = pd.read_csv('archive/sign_mnist_test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "x_train = train_df.drop('label', axis=1).values\n",
    "y_train = train_df['label'].values\n",
    "x_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Mean subtraction\n",
    "x_train = x_train - np.mean(x_train, axis=0)\n",
    "x_test = x_test - np.mean(x_test, axis=0)\n",
    "\n",
    "# Normalization \n",
    "x_train = x_train / np.std(x_train)\n",
    "x_test = x_test / np.std(x_test)\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "# Encoding the labels\n",
    "#y_train = lb.fit_transform(y_train)\n",
    "#y_test = lb.fit_transform(y_test)\n",
    "\n",
    "num_classes = 26\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Confirm preprocessing\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return x @ self.w.T + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        dw = gradient.T @ self.cur_input\n",
    "        db = gradient.sum(axis=0)\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient @ self.w\n",
    "\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x)\n",
    "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target\n",
    "    \n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, *args: List[NeuralNetLayer]):\n",
    "        self.layers = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, x, y, iterations=10, learning_rate=0.1, batch_size=26, lambda_reg=0.0):\n",
    "        history = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(iterations):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.arange(len(x))\n",
    "            np.random.shuffle(indices)\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for start_idx in range(0, len(x), batch_size):\n",
    "                # Create the mini-batch\n",
    "                end_idx = min(start_idx + batch_size, len(x))\n",
    "                x_batch = x[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.forward(x_batch)\n",
    "\n",
    "                # Loss computation with L2 regularization\n",
    "                data_loss = self.compute_loss(output, y_batch)\n",
    "                reg_loss = lambda_reg * sum(np.sum(layer.W ** 2) for layer in self.layers if hasattr(layer, 'W'))\n",
    "                loss = data_loss + reg_loss\n",
    "\n",
    "                # Backward pass with L2 gradient adjustment\n",
    "                gradients = self.backward(output, y_batch)\n",
    "                self.update_parameters(gradients, learning_rate, lambda_reg)\n",
    "\n",
    "                # Compute and store the loss and accuracy\n",
    "                history[\"loss\"].append(loss)\n",
    "                predictions = self.predict(x_batch)\n",
    "                accuracy = self.evaluate_acc(y_batch, predictions)\n",
    "                history[\"accuracy\"].append(accuracy)\n",
    "\n",
    "            # Verbose output for tracking progress\n",
    "            if (epoch + 1) % 10 == 0 or epoch == iterations - 1:\n",
    "                print(f'Epoch {epoch + 1}/{iterations} - Loss: {np.mean(history[\"loss\"][-len(X)//batch_size:])}, '\n",
    "                      f'Accuracy: {np.mean(history[\"accuracy\"][-len(x)//batch_size:])}')\n",
    "\n",
    "        return history\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate, lambda_reg):\n",
    "        for layer, gradient in zip(self.layers, gradients):\n",
    "            if hasattr(layer, 'W'):\n",
    "                layer.W -= learning_rate * (gradient[0] + lambda_reg * layer.W)\n",
    "                layer.b -= learning_rate * gradient[1]\n",
    "\n",
    "    def compute_loss(self, output, y_batch):\n",
    "        # Assuming y_batch is a one-hot encoded matrix of labels\n",
    "        m = y_batch.shape[0]  # Number of examples\n",
    "        # Clipping output to avoid division by zero\n",
    "        output_clipped = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(y_batch * np.log(output_clipped)) / m\n",
    "        return loss\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        for layer, gradient in zip(self.layers, gradients):\n",
    "            for i, param in enumerate(layer.params):\n",
    "                param -= learning_rate * gradient[i]\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicts = self.forward(x)\n",
    "        return np.argmax(predicts, axis=1)\n",
    "\n",
    "    def evaluate_acc(self, y_true, y_pred):\n",
    "        return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_and_train_mlp(x_train, y_train, x_test, y_test, hidden_layers, iterations=50, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This function builds an MLP model with the given architecture, trains it, and evaluates its performance.\n",
    "    \"\"\"\n",
    "    input_size = x_train.shape[1]  # Determine the input size from the training data\n",
    "    output_size = 10  # Assuming there are 10 classes for the Sign Language MNIST dataset\n",
    "    \n",
    "    layers = []\n",
    "    if not hidden_layers:  # Handling the no hidden layer scenario\n",
    "        layers.append(LinearLayer(input_size, output_size))\n",
    "    else:\n",
    "        for i, hidden_units in enumerate(hidden_layers):\n",
    "            if i == 0:\n",
    "                # First layer connects input to the first hidden layer\n",
    "                layers.append(LinearLayer(input_size, hidden_units))\n",
    "            else:\n",
    "                # Subsequent hidden layers\n",
    "                layers.append(LinearLayer(hidden_layers[i - 1], hidden_units))\n",
    "            # Add ReLU activation after each hidden layer\n",
    "            layers.append(ReLULayer())\n",
    "\n",
    "        # Add the output layer\n",
    "        layers.append(LinearLayer(hidden_layers[-1], output_size))\n",
    "\n",
    "    layers.append(SoftmaxOutputLayer())  # Softmax layer for classification\n",
    "    \n",
    "    # Instantiate the MLP model with the specified layers\n",
    "    mlp = MLP(*layers)\n",
    "    \n",
    "    # Train the MLP model\n",
    "    mlp.fit(x_train, y_train, iterations=10, learning_rate=0.1, batch_size=26)\n",
    "    \n",
    "    # Make predictions with the trained MLP model\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)  # Adjust based on y_test format\n",
    "    return accuracy\n",
    "\n",
    "# Experiment parameters\n",
    "hidden_units_options = [32, 64, 128, 256]\n",
    "models_architecture = [\n",
    "    [],  # Model with no hidden layer, directly mapping inputs to outputs\n",
    "    [64],  # Model with one hidden layer\n",
    "    [64, 64]  # Model with two hidden layers\n",
    "]\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "# Run experiments\n",
    "for architecture in models_architecture:\n",
    "    arch_results = []\n",
    "    for units in hidden_units_options:\n",
    "        # Adjust architecture to have the correct number of hidden units\n",
    "        adjusted_architecture = [units if len(arch) > 0 else 0 for arch in architecture]\n",
    "        accuracy = build_and_train_mlp(\n",
    "            x_train, y_train, \n",
    "            x_test, y_test,\n",
    "            adjusted_architecture\n",
    "        )\n",
    "        arch_results.append(accuracy)\n",
    "        print(f\"Architecture {adjusted_architecture}: Test Accuracy = {accuracy}\")\n",
    "    results[str(architecture)] = arch_results\n",
    "\n",
    "# Now you can compare the results stored in `results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(NeuralNetLayer):\n",
    "    def forward(self, x):\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.output * (1 - self.output))\n",
    "\n",
    "class LeakyReLULayer(NeuralNetLayer):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.ones_like(self.x)\n",
    "        dx[self.x < 0] = self.alpha\n",
    "        return dout * dx\n",
    "\n",
    "def train_model_with_activation(x_train, y_train, x_test, y_test, activation_layer, hidden_units=[64, 64]):\n",
    "    # Construct the model architecture\n",
    "    layers = [LinearLayer(x_train.shape[1], hidden_units[0]), activation_layer()]\n",
    "    for units in hidden_units[1:]:\n",
    "        layers.append(LinearLayer(units, units))\n",
    "        layers.append(activation_layer())\n",
    "    layers.append(LinearLayer(hidden_units[-1], 10))  # Assuming 10 classes\n",
    "    layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "    # Create and train the MLP model\n",
    "    mlp = MLP(*layers)\n",
    "    mlp.fit(x_train, y_train, iterations=50, learning_rate=0.01)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Activation functions to test\n",
    "activations = {\n",
    "    \"Sigmoid\": SigmoidLayer,\n",
    "    \"Leaky ReLU\": lambda: LeakyReLULayer(alpha=0.01),\n",
    "    \"ReLU\": ReLULayer  # Assuming ReLULayer is already defined\n",
    "}\n",
    "\n",
    "# Training and evaluating models with different activations\n",
    "results = {}\n",
    "for name, activation_layer in activations.items():\n",
    "    accuracy = train_model_with_activation(\n",
    "        x_train, y_train, \n",
    "        x_test, y_test, \n",
    "        activation_layer, hidden_units=[64, 64]  # Example architecture\n",
    "    )\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name} activation: Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 0.001, 0.01, 0.1, 1]  # Different values of lambda for L2 regularization\n",
    "batch_size = 64  # Or any other batch size that you want to use\n",
    "\n",
    "for lambda_reg in lambdas:\n",
    "    print(f\"Training with lambda = {lambda_reg}\")\n",
    "    mlp = MLP(\n",
    "        LinearLayer(x_train.shape[1], 64),\n",
    "        ReLULayer(),\n",
    "        LinearLayer(64, 64),\n",
    "        ReLULayer(),\n",
    "        LinearLayer(64, 10),\n",
    "        SoftmaxOutputLayer()\n",
    "    )\n",
    "    mlp.fit(x_train, y_train, iterations=50, learning_rate=0.01, batch_size=batch_size, lambda_reg=lambda_reg)\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with L2 regularization (lambda={lambda_reg}): {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the ConvNet\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, fc_units):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, fc_units)  # Assuming the images are 28x28 and pooling is applied\n",
    "        self.fc2 = nn.Linear(fc_units, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), # Ensure image is grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the dataset (adjust paths as necessary)\n",
    "train_dataset = datasets.ImageFolder(root='path_to_train_dataset', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='path_to_test_dataset', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate(hidden_units):\n",
    "    model = ConvNet(hidden_units).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):  # number of epochs can be adjusted\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\")\n",
    "\n",
    "hidden_units_options = [32, 64, 128, 256]\n",
    "for hidden_units in hidden_units_options:\n",
    "    print(f\"Training with {hidden_units} hidden units\")\n",
    "    train_and_evaluate(hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
