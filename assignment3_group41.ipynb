{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "lb=LabelBinarizer()\n",
    "\n",
    "# Load the training and test data\n",
    "train_df = pd.read_csv('archive/sign_mnist_train.csv')\n",
    "test_df = pd.read_csv('archive/sign_mnist_test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "x_train = train_df.drop('label', axis=1).values\n",
    "y_train = train_df['label'].values\n",
    "x_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Mean subtraction\n",
    "x_train = x_train - np.mean(x_train, axis=0)\n",
    "x_test = x_test - np.mean(x_test, axis=0)\n",
    "\n",
    "# Normalization \n",
    "x_train = x_train / np.std(x_train)\n",
    "x_test = x_test / np.std(x_test)\n",
    "\n",
    "x_train1 =  x_train.reshape(-1,28,28,1)\n",
    "x_test1 = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "x_train =  x_train.reshape(-1, 28*28).astype(np.float32)\n",
    "x_test = x_test.reshape(-1, 28*28).astype(np.float32)\n",
    "\n",
    "# Encoding the labels\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)\n",
    "\n",
    "# Preview the dataset\n",
    "fig,axe=plt.subplots(2,2)\n",
    "fig.suptitle('Preview of dataset')\n",
    "axe[0,0].imshow(x_train1[0].reshape(28,28),cmap='gray')\n",
    "axe[0,0].set_title('label: 3  letter: C')\n",
    "axe[0,1].imshow(x_train1[1].reshape(28,28),cmap='gray')\n",
    "axe[0,1].set_title('label: 6  letter: F')\n",
    "axe[1,0].imshow(x_train1[2].reshape(28,28),cmap='gray')\n",
    "axe[1,0].set_title('label: 2  letter: B')\n",
    "axe[1,1].imshow(x_train1[4].reshape(28,28),cmap='gray')\n",
    "axe[1,1].set_title('label: 13  letter: M')\n",
    "\n",
    "# Confirm preprocessing\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "       \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return x @ self.w.T + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        dw = gradient.T @ self.cur_input\n",
    "        db = gradient.sum(axis=0)\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient @ self.w\n",
    "\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stabilization \n",
    "        shiftx = x - np.max(x, axis=1, keepdims=True)\n",
    "        exps = np.exp(shiftx)\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.cur_probs = probs\n",
    "        return self.cur_probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, *args: List[NeuralNetLayer]):\n",
    "        self.layers = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, x, y, optimizer, iterations=10, batch_size=24, lambda_reg=0.0, verbose=True):\n",
    "        history = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(iterations):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.arange(len(x))\n",
    "            np.random.shuffle(indices)\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for start_idx in range(0, len(x), batch_size):\n",
    "                # Create the mini-batch\n",
    "                end_idx = min(start_idx + batch_size, len(x))\n",
    "                x_batch = x[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.forward(x_batch)\n",
    "\n",
    "                # Loss computation \n",
    "                data_loss = compute_loss(output, y_batch)\n",
    "                # Regularization loss\n",
    "                reg_loss = lambda_reg * sum(np.sum(layer.w ** 2) for layer in self.layers if hasattr(layer, 'w'))\n",
    "                loss = data_loss + reg_loss\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                self.backward(y_batch)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute and store the loss and accuracy\n",
    "                history[\"loss\"].append(loss)\n",
    "                predictions = self.predict(x_batch)\n",
    "                accuracy = self.evaluate_acc(y_batch, predictions)\n",
    "                history[\"accuracy\"].append(accuracy)\n",
    "                \n",
    "            # Verbose output for tracking progress\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch + 1}/{iterations} - Loss: {np.mean(history[\"loss\"][-len(x)//batch_size:])}, '\n",
    "                      f'Accuracy: {np.mean(history[\"accuracy\"][-len(x)//batch_size:])}')\n",
    "            \n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predicts = self.forward(x)\n",
    "        return np.argmax(predicts, axis=1)\n",
    "\n",
    "    def evaluate_acc(self, y_true, y_pred):\n",
    "        y_true_indices = np.argmax(y_true, axis=1)\n",
    "        return np.mean(y_true_indices == y_pred)\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, net: MLP):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "# For experiment 3\n",
    "class RegGradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float, lambda_reg: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            p -= self.lr * (g + self.lambda_reg * p)\n",
    "\n",
    "# For experiment 5            \n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.t = 0\n",
    "\n",
    "        # Initialize moment vectors for each parameter in the network\n",
    "        for layer in net.layers:\n",
    "            layer_m = []\n",
    "            layer_v = []\n",
    "            if hasattr(layer, 'parameters') and layer.parameters:\n",
    "                for param in layer.parameters:\n",
    "                    layer_m.append(np.zeros_like(param))\n",
    "                    layer_v.append(np.zeros_like(param))\n",
    "            self.m.append(layer_m)\n",
    "            self.v.append(layer_v)\n",
    "    \n",
    "    def update(self, layer_index, gradients):\n",
    "        self.t += 1\n",
    "        layer_m = self.m[layer_index]\n",
    "        layer_v = self.v[layer_index]\n",
    "        \n",
    "        # Update the parameters of the layer\n",
    "        for i, (m, v, grad) in enumerate(zip(layer_m, layer_v, gradients)):\n",
    "            if grad is not None:\n",
    "                m[:] = self.beta1 * m + (1 - self.beta1) * grad\n",
    "                v[:] = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_hat = m / (1 - self.beta1 ** self.t)\n",
    "                v_hat = v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                # Update the parameter\n",
    "                update_value = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "                self.net.layers[layer_index].parameters[i] -= update_value\n",
    "\n",
    "    # Update the parameters of the network\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.net.layers):\n",
    "            if hasattr(layer, 'parameters') and layer.parameters and layer.gradient:\n",
    "                self.update(i, layer.gradient)\n",
    "              \n",
    "# For experiment 1-3\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            p -= self.lr * g\n",
    "\n",
    "def compute_loss(output, y_batch):\n",
    "    # Assuming y_batch is a one-hot encoded matrix of labels\n",
    "    m = y_batch.shape[0]  # Number of examples\n",
    "    # Clipping output to avoid division by zero\n",
    "    output_clipped = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.sum(y_batch * np.log(output_clipped)) / m\n",
    "    return loss\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_mlp(x_train, y_train, x_test, y_test, layers, units, iterations):\n",
    "    \"\"\"\n",
    "    This function builds an MLP model with the given architecture, trains it, and evaluates its performance.\n",
    "    \"\"\"\n",
    "    input_size = x_train.shape[-1]  # Determine the input size from the training data\n",
    "    output_size = y_train.shape[-1]  # Determine the output size from the training data\n",
    "    \n",
    "    mlp_layers = []\n",
    "\n",
    "    # Add the layers to the MLP model based on the specified number of layers and units\n",
    "    if layers == 0:\n",
    "        mlp_layers.append(LinearLayer(input_size, output_size))\n",
    "    elif layers == 1:\n",
    "        mlp_layers.append(LinearLayer(input_size, units))\n",
    "        mlp_layers.append(ReLULayer())\n",
    "        mlp_layers.append(LinearLayer(units, output_size))\n",
    "    elif layers == 2:\n",
    "        mlp_layers.append(LinearLayer(input_size, units))\n",
    "        mlp_layers.append(ReLULayer())\n",
    "        mlp_layers.append(LinearLayer(units, units))\n",
    "        mlp_layers.append(ReLULayer())\n",
    "        mlp_layers.append(LinearLayer(units, output_size))\n",
    "    mlp_layers.append(SoftmaxOutputLayer())\n",
    "    \n",
    "    # Instantiate the MLP model with the specified layers\n",
    "    mlp = MLP(*mlp_layers)\n",
    "    \n",
    "    # Train the MLP model\n",
    "    optimizer = GradientDescentOptimizer(mlp, lr=0.001)\n",
    "    history = mlp.fit(x_train, y_train, optimizer, iterations=iterations)\n",
    "    \n",
    "    # Make predictions with the trained MLP model\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = mlp.evaluate_acc(y_test, y_pred)  # Adjust based on y_test format\n",
    "\n",
    "    return accuracy, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_1(hidden_units, hidden_layers, iterations, plot_show=False):\n",
    "   \n",
    "    # Results dictionary to store the accuracy for each architecture and unit configuration\n",
    "    results = {}\n",
    "    max_accuracy = 0\n",
    "    max_accuracy_units = []\n",
    "    max_accuracy_layers = []\n",
    "    # Run experiments\n",
    "    for layers in hidden_layers:\n",
    "        arch_results = []\n",
    "        for units in hidden_units:\n",
    "            # If the architecture has hidden layers (is not empty)\n",
    "            \n",
    "            if layers == 0:\n",
    "                num_layers = \"No Hidden Layer\"\n",
    "            elif layers == 1:\n",
    "                num_layers = \"One Hidden Layer\"\n",
    "            elif layers == 2:\n",
    "                num_layers = \"Two Hidden Layers\"\n",
    "\n",
    "            # Print the architecture being tested for clarity\n",
    "            print(f\"Testing architecture: {num_layers}\")\n",
    "            \n",
    "            # Train the model and evaluate accuracy\n",
    "            accuracy, history = build_and_train_mlp(\n",
    "                x_train, y_train, \n",
    "                x_test, y_test,\n",
    "                layers, units,\n",
    "                iterations\n",
    "            )\n",
    "            \n",
    "            # Store and print the accuracy for the current configuration\n",
    "            arch_results.append((units, accuracy))\n",
    "            if accuracy > max_accuracy:\n",
    "                max_accuracy = accuracy\n",
    "                max_accuracy_units = units\n",
    "                max_accuracy_layers = num_layers\n",
    "\n",
    "            if plot_show:\n",
    "                # Plot the training loss\n",
    "                plt.plot(history['loss'])\n",
    "                plt.title('Training Loss for ' + num_layers + ' Layers and ' + str(units) + ' Units')\n",
    "                plt.xlabel('Iterations')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.show()\n",
    "            \n",
    "            print(f\"Units: {units}, Test Accuracy: {accuracy}\\n\")\n",
    "            \n",
    "        # Store the results for the current architecture\n",
    "        results[str(layers)] = arch_results\n",
    "    print(f\"Max Accuracy: {max_accuracy}, for Units: {max_accuracy_units} and Layers: {max_accuracy_layers}\")\n",
    "\n",
    "# Experiment parameters\n",
    "hidden_units = [32, 64, 128, 256]\n",
    "hidden_layers = [0,1,2]\n",
    "    \n",
    "experiment_1(hidden_units, hidden_layers, iterations=10, plot_show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(NeuralNetLayer):\n",
    "    def forward(self, x):\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.output * (1 - self.output))\n",
    "\n",
    "class LeakyReLULayer(NeuralNetLayer):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.ones_like(self.x)\n",
    "        dx[self.x < 0] = self.alpha\n",
    "        return dout * dx\n",
    "\n",
    "def experiment_2(x_train, y_train, x_test, y_test, activation_layer, hidden_units=[64, 64]):\n",
    "    # Construct the model architecture\n",
    "    layers = [LinearLayer(x_train.shape[-1], hidden_units[0]), activation_layer()]\n",
    "    for units in hidden_units[1:]:\n",
    "        layers.append(LinearLayer(units, units))\n",
    "        layers.append(activation_layer())\n",
    "    layers.append(LinearLayer(hidden_units[-1], y_train.shape[-1]))  \n",
    "    layers.append(SoftmaxOutputLayer())\n",
    "    \n",
    "\n",
    "    # Create and train the MLP model\n",
    "    mlp = MLP(*layers)\n",
    "    optimizer = GradientDescentOptimizer(mlp, lr=0.001)\n",
    "    mlp.fit(x_train, y_train, optimizer, iterations=10)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    accuracy = mlp.evaluate_acc(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Activation functions to test\n",
    "activations = {\n",
    "    \"Sigmoid\": SigmoidLayer,\n",
    "    \"Leaky ReLU\": LeakyReLULayer,\n",
    "    \"ReLU\": ReLULayer  \n",
    "}\n",
    "\n",
    "# Training and evaluating models with different activations\n",
    "results = {}\n",
    "for name, activation_layer in activations.items():\n",
    "    accuracy = experiment_2(\n",
    "        x_train, y_train, \n",
    "        x_test, y_test, \n",
    "        activation_layer, hidden_units=[64, 64] # 2 hidden layers with 64 units each\n",
    "    )\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name} activation: Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 0.001, 0.01, 0.1, 1]  # Different values of lambda for L2 regularization\n",
    "batch_size = 24 # Mini-batch size\n",
    "units = 32 # Number of units in the hidden layers\n",
    "for lambda_reg in lambdas:\n",
    "    print(f\"Training with lambda = {lambda_reg} and units = {units}\")\n",
    "    mlp = MLP(\n",
    "        LinearLayer(x_train.shape[-1], units),\n",
    "        ReLULayer(),\n",
    "        LinearLayer(units, units),\n",
    "        ReLULayer(),\n",
    "        LinearLayer(units, y_train.shape[-1]),\n",
    "        SoftmaxOutputLayer()\n",
    "    )\n",
    "    optimizer = RegGradientDescentOptimizer(mlp, lr=0.001, lambda_reg=lambda_reg)\n",
    "    history = mlp.fit(x_train, y_train, optimizer, iterations=10, batch_size=batch_size, lambda_reg=lambda_reg)\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    accuracy = mlp.evaluate_acc(y_test, y_pred)\n",
    "    print(f\"Accuracy with L2 regularization and lambda={lambda_reg}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('archive/sign_mnist_train.csv')\n",
    "test_df = pd.read_csv('archive/sign_mnist_test.csv')\n",
    "\n",
    "# Extract labels and features\n",
    "y_train = train_df.pop('label').values\n",
    "x_train = train_df.values\n",
    "y_test = test_df.pop('label').values\n",
    "x_test = test_df.values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train).astype(np.float32)\n",
    "x_test = scaler.transform(x_test).astype(np.float32)\n",
    "\n",
    "# Reshape the features\n",
    "x_train = x_train.reshape((-1, 1, 28, 28))\n",
    "x_test = x_test.reshape((-1, 1, 28, 28))\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "x_test_tensor = torch.tensor(x_test)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the ConvNet model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, hidden_units)  # 3x3 is the spatial dimension of the output volume\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_units, 26)  # 26 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate_model(hidden_units):\n",
    "    model = ConvNet(hidden_units)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(10):  # 10 iterations\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.argmax(dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "\n",
    "    test_acc = correct / total\n",
    "    return test_acc\n",
    "\n",
    "# Running the experiments\n",
    "hidden_units_options = [32, 64, 128, 256]\n",
    "results = {}\n",
    "for hidden_units in hidden_units_options:\n",
    "    test_acc = train_and_evaluate_model(hidden_units)\n",
    "    results[hidden_units] = test_acc\n",
    "    print(f\"Training with {hidden_units} hidden units, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#Find the best performance\n",
    "best_units = max(results, key=results.get)\n",
    "print(f\"Best performance with {best_units} hidden units: {results[best_units]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE TO RUN THE FIRST PREPROCESSING CELL RIGHT BEFORE RUNNING THIS CELL \n",
    "# TO MAKE SURE THE DATA IS IN THE CORRECT FORMAT\n",
    "# THE ORDER IS EXPERIMENT 4, PREPROCESSING, EXPERIMENT 5\n",
    "\n",
    "# Define the MLP architecture \n",
    "units = 256 # If you want to skip running the previous cell you can use 128 or 256\n",
    "input_size = x_train.shape[-1]  # Determine the input size from the training data\n",
    "output_size = y_train.shape[-1]  # Determine the output size from the training data\n",
    "mlp = MLP(\n",
    "    LinearLayer(input_size, units),  \n",
    "    ReLULayer(),            \n",
    "    LinearLayer(units, units),  \n",
    "    ReLULayer(),            \n",
    "    LinearLayer(units, units),  \n",
    "    ReLULayer(),            \n",
    "    LinearLayer(units, output_size),   \n",
    "    SoftmaxOutputLayer()    \n",
    ")\n",
    "\n",
    "# Initialize the Optimizer \n",
    "adam_optimizer = AdamOptimizer(mlp, lr=0.001)\n",
    "\n",
    "# Train the MLP model\n",
    "mlp.fit(x_train, y_train, optimizer=adam_optimizer, iterations=10, verbose=True)\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "y_pred = mlp.predict(x_test)\n",
    "accuracy = mlp.evaluate_acc(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
