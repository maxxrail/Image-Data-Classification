{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (27455, 784)\n",
      "Test data shape: (7172, 784)\n",
      "Training labels shape: (27455, 24)\n",
      "Test labels shape: (7172, 24)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocessing(visualize=False):\n",
    "    lb=LabelBinarizer()\n",
    "\n",
    "    # Load the training and test data\n",
    "    train_df = pd.read_csv('archive/sign_mnist_train.csv')\n",
    "    test_df = pd.read_csv('archive/sign_mnist_test.csv')\n",
    "\n",
    "    # Separate features and labels\n",
    "    x_train = train_df.drop('label', axis=1).values\n",
    "    y_train = train_df['label'].values\n",
    "    x_test = test_df.drop('label', axis=1).values\n",
    "    y_test = test_df['label'].values\n",
    "\n",
    "    # Mean subtraction\n",
    "    x_train = x_train - np.mean(x_train, axis=0)\n",
    "    x_test = x_test - np.mean(x_test, axis=0)\n",
    "\n",
    "    # Normalization \n",
    "    x_train = x_train / np.std(x_train)\n",
    "    x_test = x_test / np.std(x_test)\n",
    "\n",
    "    x_train1 =  x_train.reshape(-1,28,28,1)\n",
    "    x_test1 = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "    x_train =  x_train.reshape(-1, 28*28).astype(np.float32)\n",
    "    x_test = x_test.reshape(-1, 28*28).astype(np.float32)\n",
    "\n",
    "    # Encoding the labels\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.fit_transform(y_test)\n",
    "\n",
    "    if (visualize==True):\n",
    "    # Preview the dataset\n",
    "        fig,axe=plt.subplots(2,2)\n",
    "        fig.suptitle('Preview of dataset')\n",
    "        axe[0,0].imshow(x_train1[0].reshape(28,28),cmap='gray')\n",
    "        axe[0,0].set_title('label: 3  letter: C')\n",
    "        axe[0,1].imshow(x_train1[1].reshape(28,28),cmap='gray')\n",
    "        axe[0,1].set_title('label: 6  letter: F')\n",
    "        axe[1,0].imshow(x_train1[2].reshape(28,28),cmap='gray')\n",
    "        axe[1,0].set_title('label: 2  letter: B')\n",
    "        axe[1,1].imshow(x_train1[4].reshape(28,28),cmap='gray')\n",
    "        axe[1,1].set_title('label: 13  letter: M')\n",
    "\n",
    "        # Confirm preprocessing\n",
    "        print(\"Training data shape:\", x_train.shape)\n",
    "        print(\"Test data shape:\", x_test.shape)\n",
    "        print(\"Training labels shape:\", y_train.shape)\n",
    "        print(\"Test labels shape:\", y_test.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "    \n",
    "x_train, y_train, x_test, y_test = preprocessing(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "       \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return x @ self.w.T + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        dw = gradient.T @ self.cur_input\n",
    "        db = gradient.sum(axis=0)\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient @ self.w\n",
    "\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stabilization \n",
    "        shiftx = x - np.max(x, axis=1, keepdims=True)\n",
    "        exps = np.exp(shiftx)\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.cur_probs = probs\n",
    "        return self.cur_probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, *args: List[NeuralNetLayer]):\n",
    "        self.layers = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, x, y, optimizer, iterations=10, batch_size=24, lambda_reg=0.0, verbose=True):\n",
    "        history = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(iterations):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.arange(len(x))\n",
    "            np.random.shuffle(indices)\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for start_idx in range(0, len(x), batch_size):\n",
    "                # Create the mini-batch\n",
    "                end_idx = min(start_idx + batch_size, len(x))\n",
    "                x_batch = x[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.forward(x_batch)\n",
    "\n",
    "                # Loss computation \n",
    "                data_loss = compute_loss(output, y_batch)\n",
    "                # Regularization loss\n",
    "                reg_loss = lambda_reg * sum(np.sum(layer.w ** 2) for layer in self.layers if hasattr(layer, 'w'))\n",
    "                loss = data_loss + reg_loss\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                self.backward(y_batch)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute and store the loss and accuracy\n",
    "                history[\"loss\"].append(loss)\n",
    "                predictions = self.predict(x_batch)\n",
    "                accuracy = self.evaluate_acc(y_batch, predictions)\n",
    "                history[\"accuracy\"].append(accuracy)\n",
    "                \n",
    "            # Verbose output for tracking progress\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch + 1}/{iterations} - Loss: {np.mean(history[\"loss\"][-len(x)//batch_size:])}, '\n",
    "                      f'Accuracy: {np.mean(history[\"accuracy\"][-len(x)//batch_size:])}')\n",
    "            \n",
    "\n",
    "        return history\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predicts = self.forward(x)\n",
    "        return np.argmax(predicts, axis=1)\n",
    "\n",
    "    def evaluate_acc(self, y_true, y_pred):\n",
    "        y_true_indices = np.argmax(y_true, axis=1)\n",
    "        return np.mean(y_true_indices == y_pred)\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, net: MLP):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "# For experiment 3\n",
    "class RegGradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float, lambda_reg: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            p -= self.lr * (g + self.lambda_reg * p)\n",
    "\n",
    "# For experiment 5            \n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.t = 0\n",
    "\n",
    "        # Initialize moment vectors for each parameter in the network\n",
    "        for layer in net.layers:\n",
    "            layer_m = []\n",
    "            layer_v = []\n",
    "            if hasattr(layer, 'parameters') and layer.parameters:\n",
    "                for param in layer.parameters:\n",
    "                    layer_m.append(np.zeros_like(param))\n",
    "                    layer_v.append(np.zeros_like(param))\n",
    "            self.m.append(layer_m)\n",
    "            self.v.append(layer_v)\n",
    "    \n",
    "    def update(self, layer_index, gradients):\n",
    "        self.t += 1\n",
    "        layer_m = self.m[layer_index]\n",
    "        layer_v = self.v[layer_index]\n",
    "        \n",
    "        # Update the parameters of the layer\n",
    "        for i, (m, v, grad) in enumerate(zip(layer_m, layer_v, gradients)):\n",
    "            if grad is not None:\n",
    "                m[:] = self.beta1 * m + (1 - self.beta1) * grad\n",
    "                v[:] = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_hat = m / (1 - self.beta1 ** self.t)\n",
    "                v_hat = v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                # Update the parameter\n",
    "                update_value = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "                self.net.layers[layer_index].parameters[i] -= update_value\n",
    "\n",
    "    # Update the parameters of the network\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.net.layers):\n",
    "            if hasattr(layer, 'parameters') and layer.parameters and layer.gradient:\n",
    "                self.update(i, layer.gradient)\n",
    "              \n",
    "# For experiment 1-3\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            p -= self.lr * g\n",
    "\n",
    "def compute_loss(output, y_batch):\n",
    "    # Assuming y_batch is a one-hot encoded matrix of labels\n",
    "    m = y_batch.shape[0]  # Number of examples\n",
    "    # Clipping output to avoid division by zero\n",
    "    output_clipped = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.sum(y_batch * np.log(output_clipped)) / m\n",
    "    return loss\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_mlp(x_train, y_train, x_test, y_test, layers, units, iterations):\n",
    "    \"\"\"\n",
    "    This function builds an MLP model with the given architecture, trains it, and evaluates its performance.\n",
    "    \"\"\"\n",
    "    input_size = x_train.shape[-1]  # Determine the input size from the training data\n",
    "    output_size = y_train.shape[-1]  # Determine the output size from the training data\n",
    "    \n",
    "    mlp_layers = []\n",
    "\n",
    "    # Add the layers to the MLP model based on the specified number of layers and units\n",
    "    if layers == 0:\n",
    "        mlp_layers.append(LinearLayer(input_size, output_size))\n",
    "    elif layers == 1:\n",
    "        mlp_layers.append(LinearLayer(input_size, units))\n",
    "        mlp_layers.append(ReLULayer())\n",
    "        mlp_layers.append(LinearLayer(units, output_size))\n",
    "    elif layers == 2:\n",
    "        mlp_layers.append(LinearLayer(input_size, units))\n",
    "        mlp_layers.append(ReLULayer())\n",
    "        mlp_layers.append(LinearLayer(units, units))\n",
    "        mlp_layers.append(ReLULayer())\n",
    "        mlp_layers.append(LinearLayer(units, output_size))\n",
    "    mlp_layers.append(SoftmaxOutputLayer())\n",
    "    \n",
    "    # Instantiate the MLP model with the specified layers\n",
    "    mlp = MLP(*mlp_layers)\n",
    "    \n",
    "    # Train the MLP model\n",
    "    optimizer = GradientDescentOptimizer(mlp, lr=0.001)\n",
    "    history = mlp.fit(x_train, y_train, optimizer, iterations=iterations)\n",
    "    \n",
    "    # Make predictions with the trained MLP model\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = mlp.evaluate_acc(y_test, y_pred)  # Adjust based on y_test format\n",
    "\n",
    "    return accuracy, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing architecture: No Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.8726975812335487, Accuracy: 0.8698977652782001\n",
      "Epoch 2/10 - Loss: 0.3236030853324885, Accuracy: 0.9759615384615384\n",
      "Epoch 3/10 - Loss: 0.1995864307523761, Accuracy: 0.9936990093240093\n",
      "Epoch 4/10 - Loss: 0.14150419546037454, Accuracy: 0.9985067016317015\n",
      "Epoch 5/10 - Loss: 0.10870259197097155, Accuracy: 0.9994900932400931\n",
      "Epoch 6/10 - Loss: 0.08868404403699286, Accuracy: 0.999635780885781\n",
      "Epoch 7/10 - Loss: 0.07473199966668728, Accuracy: 0.9997086247086248\n",
      "Epoch 8/10 - Loss: 0.0646302805738914, Accuracy: 0.9998178904428905\n",
      "Epoch 9/10 - Loss: 0.057037971000125656, Accuracy: 0.9998543123543124\n",
      "Epoch 10/10 - Loss: 0.05103705915134059, Accuracy: 0.9998907342657343\n",
      "Units: 32, Test Accuracy: 0.6784718349135527\n",
      "\n",
      "Testing architecture: No Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.8700393288037873, Accuracy: 0.8715763403263403\n",
      "Epoch 2/10 - Loss: 0.31852322495335134, Accuracy: 0.9780011655011654\n",
      "Epoch 3/10 - Loss: 0.19784244679885757, Accuracy: 0.9937718531468531\n",
      "Epoch 4/10 - Loss: 0.1403211594678417, Accuracy: 0.9982881701631701\n",
      "Epoch 5/10 - Loss: 0.10828011918877842, Accuracy: 0.9994536713286714\n",
      "Epoch 6/10 - Loss: 0.08806836946487011, Accuracy: 0.9996722027972028\n",
      "Epoch 7/10 - Loss: 0.07436446066759399, Accuracy: 0.9997086247086245\n",
      "Epoch 8/10 - Loss: 0.06427115270565044, Accuracy: 0.9998178904428903\n",
      "Epoch 9/10 - Loss: 0.05671143916676595, Accuracy: 0.9998543123543124\n",
      "Epoch 10/10 - Loss: 0.050795090057287856, Accuracy: 0.9998543123543123\n",
      "Units: 64, Test Accuracy: 0.6885108756274401\n",
      "\n",
      "Testing architecture: No Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.8874855391670554, Accuracy: 0.8667670644572819\n",
      "Epoch 2/10 - Loss: 0.3213922717150345, Accuracy: 0.9765442890442889\n",
      "Epoch 3/10 - Loss: 0.19797859191958092, Accuracy: 0.992606351981352\n",
      "Epoch 4/10 - Loss: 0.14014693003974313, Accuracy: 0.9981060606060608\n",
      "Epoch 5/10 - Loss: 0.10784564562782047, Accuracy: 0.9995265151515152\n",
      "Epoch 6/10 - Loss: 0.08771240875264444, Accuracy: 0.9997814685314685\n",
      "Epoch 7/10 - Loss: 0.07396855792957754, Accuracy: 0.9997450466200466\n",
      "Epoch 8/10 - Loss: 0.06414937119456578, Accuracy: 0.9998178904428905\n",
      "Epoch 9/10 - Loss: 0.056591102341733285, Accuracy: 0.9998543123543124\n",
      "Epoch 10/10 - Loss: 0.05061185378677351, Accuracy: 0.9998543123543123\n",
      "Units: 128, Test Accuracy: 0.6823759063022866\n",
      "\n",
      "Testing architecture: No Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.8824849612815732, Accuracy: 0.8683332066484242\n",
      "Epoch 2/10 - Loss: 0.3233230663244902, Accuracy: 0.9771270396270397\n",
      "Epoch 3/10 - Loss: 0.19946305591367702, Accuracy: 0.9935533216783217\n",
      "Epoch 4/10 - Loss: 0.14120027788947168, Accuracy: 0.9975233100233099\n",
      "Epoch 5/10 - Loss: 0.10917116937274306, Accuracy: 0.9992715617715618\n",
      "Epoch 6/10 - Loss: 0.08881309183809313, Accuracy: 0.9996357808857809\n",
      "Epoch 7/10 - Loss: 0.07479929907114698, Accuracy: 0.9996722027972028\n",
      "Epoch 8/10 - Loss: 0.06480455322052728, Accuracy: 0.9997086247086248\n",
      "Epoch 9/10 - Loss: 0.05714268676787628, Accuracy: 0.9998543123543124\n",
      "Epoch 10/10 - Loss: 0.0510377006868041, Accuracy: 0.999927156177156\n",
      "Units: 256, Test Accuracy: 0.6728945900725042\n",
      "\n",
      "Testing architecture: One Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.8607524347170172, Accuracy: 0.8705565268065268\n",
      "Epoch 2/10 - Loss: 0.17040994902893858, Accuracy: 0.9943546037296036\n",
      "Epoch 3/10 - Loss: 0.06857498671957836, Accuracy: 0.9994536713286714\n",
      "Epoch 4/10 - Loss: 0.03815951364412253, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.025378217058830567, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.018746127775610917, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.01469770820740006, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.01199618057497995, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.010103524971567723, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.008691719773638378, Accuracy: 1.0\n",
      "Units: 32, Test Accuracy: 0.714026770775237\n",
      "\n",
      "Testing architecture: One Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.6726039566591785, Accuracy: 0.9211465617715618\n",
      "Epoch 2/10 - Loss: 0.09976330915040593, Accuracy: 0.9996722027972028\n",
      "Epoch 3/10 - Loss: 0.041934160943140744, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.02515309837462849, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.017544048268475207, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.013350415481226901, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.010668557400061658, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.00886560846850958, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.007553616267226272, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.006560389274829754, Accuracy: 1.0\n",
      "Units: 64, Test Accuracy: 0.7532069157836029\n",
      "\n",
      "Testing architecture: One Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.6096528183421259, Accuracy: 0.9375\n",
      "Epoch 2/10 - Loss: 0.08128637236295855, Accuracy: 0.9999635780885783\n",
      "Epoch 3/10 - Loss: 0.03490758116016655, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.021291185911570497, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.015035605390356085, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.01151265277979591, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.009269973067325238, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0077277964790288586, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.006599749512500841, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.005742637050596684, Accuracy: 1.0\n",
      "Units: 128, Test Accuracy: 0.7749581706636921\n",
      "\n",
      "Testing architecture: One Hidden Layer\n",
      "Epoch 1/10 - Loss: 0.5659800472550666, Accuracy: 0.9472246503496503\n",
      "Epoch 2/10 - Loss: 0.07028485090078217, Accuracy: 0.9999271561771562\n",
      "Epoch 3/10 - Loss: 0.031029650586875665, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.01924583950517384, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.013699357490681302, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.010540807975275863, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.008520146052893408, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0071124732691671255, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.006083768035676234, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.00531382480060613, Accuracy: 1.0\n",
      "Units: 256, Test Accuracy: 0.7836029001673174\n",
      "\n",
      "Testing architecture: Two Hidden Layers\n",
      "Epoch 1/10 - Loss: 0.8517512975520751, Accuracy: 0.8731773208675383\n",
      "Epoch 2/10 - Loss: 0.11222295347157546, Accuracy: 0.9982881701631701\n",
      "Epoch 3/10 - Loss: 0.03129023176369898, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.015492987564101007, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.009924674471760079, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.00712457410867053, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.00545890577121802, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.004423683797106374, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.003675712923789737, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.0031515787030744788, Accuracy: 1.0\n",
      "Units: 32, Test Accuracy: 0.6866982710540993\n",
      "\n",
      "Testing architecture: Two Hidden Layers\n",
      "Epoch 1/10 - Loss: 0.638078125316728, Accuracy: 0.9266462703962706\n",
      "Epoch 2/10 - Loss: 0.04660371931080515, Accuracy: 1.0\n",
      "Epoch 3/10 - Loss: 0.016339697954430625, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.009335442933107313, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.0064333500909216305, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.00481196143511552, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.00380943754177881, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0031504894440181033, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.0026638781734094966, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.0023028381623522187, Accuracy: 1.0\n",
      "Units: 64, Test Accuracy: 0.7763524818739542\n",
      "\n",
      "Testing architecture: Two Hidden Layers\n",
      "Epoch 1/10 - Loss: 0.5256126652014975, Accuracy: 0.9516317016317015\n",
      "Epoch 2/10 - Loss: 0.03166143245148758, Accuracy: 1.0\n",
      "Epoch 3/10 - Loss: 0.01269400306547355, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.007658765737962126, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.005381413858486168, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.004113931944448772, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.0033020398066825516, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0027449880074014686, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.0023443919791198654, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.0020331409147486697, Accuracy: 1.0\n",
      "Units: 128, Test Accuracy: 0.7894590072504183\n",
      "\n",
      "Testing architecture: Two Hidden Layers\n",
      "Epoch 1/10 - Loss: 0.45107440386296055, Accuracy: 0.9678758741258742\n",
      "Epoch 2/10 - Loss: 0.024988056434535888, Accuracy: 1.0\n",
      "Epoch 3/10 - Loss: 0.010905593019983506, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.006780714365930598, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.004840514892140912, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.003727760904961654, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.0030138328730694385, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0025233324873468774, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.002158684947296575, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.001883087831661931, Accuracy: 1.0\n",
      "Units: 256, Test Accuracy: 0.7908533184606804\n",
      "\n",
      "Max Accuracy: 0.7908533184606804, for Units: 256 and Layers: Two Hidden Layers\n"
     ]
    }
   ],
   "source": [
    "def experiment_1(hidden_units, hidden_layers, iterations, plot_show=False):\n",
    "   \n",
    "    # Results dictionary to store the accuracy for each architecture and unit configuration\n",
    "    results = {}\n",
    "    max_accuracy = 0\n",
    "    max_accuracy_units = []\n",
    "    max_accuracy_layers = []\n",
    "    # Run experiments\n",
    "    for layers in hidden_layers:\n",
    "        arch_results = []\n",
    "        for units in hidden_units:\n",
    "            # If the architecture has hidden layers (is not empty)\n",
    "            \n",
    "            if layers == 0:\n",
    "                num_layers = \"No Hidden Layer\"\n",
    "            elif layers == 1:\n",
    "                num_layers = \"One Hidden Layer\"\n",
    "            elif layers == 2:\n",
    "                num_layers = \"Two Hidden Layers\"\n",
    "\n",
    "            # Print the architecture being tested for clarity\n",
    "            print(f\"Testing architecture: {num_layers}\")\n",
    "            \n",
    "            # Train the model and evaluate accuracy\n",
    "            accuracy, history = build_and_train_mlp(\n",
    "                x_train, y_train, \n",
    "                x_test, y_test,\n",
    "                layers, units,\n",
    "                iterations\n",
    "            )\n",
    "            \n",
    "            # Store and print the accuracy for the current configuration\n",
    "            arch_results.append((units, accuracy))\n",
    "            if accuracy > max_accuracy:\n",
    "                max_accuracy = accuracy\n",
    "                max_accuracy_units = units\n",
    "                max_accuracy_layers = num_layers\n",
    "\n",
    "            if plot_show:\n",
    "                # Plot the training loss\n",
    "                plt.plot(history['loss'])\n",
    "                plt.title('Training Loss for ' + num_layers + ' Layers and ' + str(units) + ' Units')\n",
    "                plt.xlabel('Iterations')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.show()\n",
    "            \n",
    "            print(f\"Units: {units}, Test Accuracy: {accuracy}\\n\")\n",
    "            \n",
    "        # Store the results for the current architecture\n",
    "        results[str(layers)] = arch_results\n",
    "    print(f\"Max Accuracy: {max_accuracy}, for Units: {max_accuracy_units} and Layers: {max_accuracy_layers}\")\n",
    "\n",
    "# Experiment parameters\n",
    "hidden_units = [32, 64, 128, 256]\n",
    "hidden_layers = [0,1,2]\n",
    "    \n",
    "experiment_1(hidden_units, hidden_layers, iterations=10, plot_show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 2.8649404768563898, Accuracy: 0.24527782000608087\n",
      "Epoch 2/10 - Loss: 2.1534973505570423, Accuracy: 0.4707722078645992\n",
      "Epoch 3/10 - Loss: 1.657612548231506, Accuracy: 0.6140940128711868\n",
      "Epoch 4/10 - Loss: 1.3131623263676275, Accuracy: 0.6987036966656532\n",
      "Epoch 5/10 - Loss: 1.0550428564755234, Accuracy: 0.7635410332421202\n",
      "Epoch 6/10 - Loss: 0.8576720881312373, Accuracy: 0.8189039221647917\n",
      "Epoch 7/10 - Loss: 0.7018203285351537, Accuracy: 0.8679658204114725\n",
      "Epoch 8/10 - Loss: 0.5754732887803187, Accuracy: 0.9072333916083916\n",
      "Epoch 9/10 - Loss: 0.4716246769907052, Accuracy: 0.9370993589743588\n",
      "Epoch 10/10 - Loss: 0.38601935776239704, Accuracy: 0.9564726487280835\n",
      "Sigmoid activation: Test Accuracy = 0.701617401003904\n",
      "Epoch 1/10 - Loss: 0.6506640441413611, Accuracy: 0.9192526223776224\n",
      "Epoch 2/10 - Loss: 0.049694578772420187, Accuracy: 1.0\n",
      "Epoch 3/10 - Loss: 0.01717115374707515, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.009834797804239091, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.006691912676716428, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.005012576109540622, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.003975288494367063, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0032644432066974314, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.002767877076778004, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.002388017763381389, Accuracy: 1.0\n",
      "Leaky ReLU activation: Test Accuracy = 0.7488845510317903\n",
      "Epoch 1/10 - Loss: 0.6613345132436168, Accuracy: 0.9172129953379954\n",
      "Epoch 2/10 - Loss: 0.04729811170208286, Accuracy: 1.0\n",
      "Epoch 3/10 - Loss: 0.01656408652759127, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.009553590731356965, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.006553203940183207, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.004917138391854543, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.0039025860825660603, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.0032158402459191012, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.002721852651642, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.002350551804061574, Accuracy: 1.0\n",
      "ReLU activation: Test Accuracy = 0.7491634132738427\n"
     ]
    }
   ],
   "source": [
    "class SigmoidLayer(NeuralNetLayer):\n",
    "    def forward(self, x):\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.output * (1 - self.output))\n",
    "\n",
    "class LeakyReLULayer(NeuralNetLayer):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.ones_like(self.x)\n",
    "        dx[self.x < 0] = self.alpha\n",
    "        return dout * dx\n",
    "\n",
    "def experiment_2(x_train, y_train, x_test, y_test, activation_layer, hidden_units=[64, 64]):\n",
    "    # Construct the model architecture\n",
    "    layers = [LinearLayer(x_train.shape[-1], hidden_units[0]), activation_layer()]\n",
    "    for units in hidden_units[1:]:\n",
    "        layers.append(LinearLayer(units, units))\n",
    "        layers.append(activation_layer())\n",
    "    layers.append(LinearLayer(hidden_units[-1], y_train.shape[-1]))  \n",
    "    layers.append(SoftmaxOutputLayer())\n",
    "    \n",
    "\n",
    "    # Create and train the MLP model\n",
    "    mlp = MLP(*layers)\n",
    "    optimizer = GradientDescentOptimizer(mlp, lr=0.001)\n",
    "    mlp.fit(x_train, y_train, optimizer, iterations=10)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    accuracy = mlp.evaluate_acc(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Activation functions to test\n",
    "activations = {\n",
    "    \"Sigmoid\": SigmoidLayer,\n",
    "    \"Leaky ReLU\": LeakyReLULayer,\n",
    "    \"ReLU\": ReLULayer  \n",
    "}\n",
    "\n",
    "# Training and evaluating models with different activations\n",
    "results = {}\n",
    "for name, activation_layer in activations.items():\n",
    "    accuracy = experiment_2(\n",
    "        x_train, y_train, \n",
    "        x_test, y_test, \n",
    "        activation_layer, hidden_units=[64, 64] # 2 hidden layers with 64 units each\n",
    "    )\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name} activation: Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lambda = 0 and units = 32\n",
      "Epoch 1/10 - Loss: 0.7661845087271042, Accuracy: 0.900458916083916\n",
      "Epoch 2/10 - Loss: 0.07993919993125036, Accuracy: 0.9993079836829836\n",
      "Epoch 3/10 - Loss: 0.024921689528813164, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.013353949368909318, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.008894345207815268, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.00646852061233511, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.005082045652681833, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.004117897271900618, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.003477859808377118, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.0029746354480766103, Accuracy: 1.0\n",
      "Accuracy with L2 regularization and lambda=0: 0.7374511991076408\n",
      "Training with lambda = 0.001 and units = 32\n",
      "Epoch 1/10 - Loss: 1.0574366807064042, Accuracy: 0.8726325757575756\n",
      "Epoch 2/10 - Loss: 0.34413961823807215, Accuracy: 0.9985067016317017\n",
      "Epoch 3/10 - Loss: 0.29786934623260475, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 0.2960726695815148, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 0.29862043391798393, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 0.3015313777332182, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 0.30412109492187017, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 0.3064673365161963, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 0.30853703934444554, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 0.3103863280132843, Accuracy: 1.0\n",
      "Accuracy with L2 regularization and lambda=0.001: 0.6999442275515895\n",
      "Training with lambda = 0.01 and units = 32\n",
      "Epoch 1/10 - Loss: 2.9767598377851128, Accuracy: 0.8515062835715009\n",
      "Epoch 2/10 - Loss: 2.5894356262981253, Accuracy: 0.9985795454545454\n",
      "Epoch 3/10 - Loss: 2.7024166362093487, Accuracy: 1.0\n",
      "Epoch 4/10 - Loss: 2.7681948157354235, Accuracy: 0.9999635780885783\n",
      "Epoch 5/10 - Loss: 2.800681368092413, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 2.8140609338086224, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 2.8167329333595172, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 2.8121627340450406, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 2.8030378748180014, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 2.790503103779804, Accuracy: 1.0\n",
      "Accuracy with L2 regularization and lambda=0.01: 0.6882320133853876\n",
      "Training with lambda = 0.1 and units = 32\n",
      "Epoch 1/10 - Loss: 19.645827780423826, Accuracy: 0.8728859455761631\n",
      "Epoch 2/10 - Loss: 20.04377504238387, Accuracy: 0.9981789044289043\n",
      "Epoch 3/10 - Loss: 19.240761589768862, Accuracy: 0.999963578088578\n",
      "Epoch 4/10 - Loss: 18.03981650122894, Accuracy: 1.0\n",
      "Epoch 5/10 - Loss: 16.936623196295674, Accuracy: 1.0\n",
      "Epoch 6/10 - Loss: 16.014582706953828, Accuracy: 1.0\n",
      "Epoch 7/10 - Loss: 15.277997817406593, Accuracy: 1.0\n",
      "Epoch 8/10 - Loss: 14.684701265738113, Accuracy: 1.0\n",
      "Epoch 9/10 - Loss: 14.214878060187683, Accuracy: 1.0\n",
      "Epoch 10/10 - Loss: 13.824358906673911, Accuracy: 1.0\n",
      "Accuracy with L2 regularization and lambda=0.1: 0.6910206358059119\n",
      "Training with lambda = 1 and units = 32\n",
      "Epoch 1/10 - Loss: 95.12603416629278, Accuracy: 0.7591418997668997\n",
      "Epoch 2/10 - Loss: 54.96905578331971, Accuracy: 0.9066474739029087\n",
      "Epoch 3/10 - Loss: 51.89240624045232, Accuracy: 0.9328364117766289\n",
      "Epoch 4/10 - Loss: 51.90774918134183, Accuracy: 0.9474431818181818\n",
      "Epoch 5/10 - Loss: 52.189874009856545, Accuracy: 0.9561448515252863\n",
      "Epoch 6/10 - Loss: 52.27050164782337, Accuracy: 0.9593895687645688\n",
      "Epoch 7/10 - Loss: 52.371623725542804, Accuracy: 0.9631742804297152\n",
      "Epoch 8/10 - Loss: 52.443514758096384, Accuracy: 0.9660912004662005\n",
      "Epoch 9/10 - Loss: 52.41854132514392, Accuracy: 0.9679107124759299\n",
      "Epoch 10/10 - Loss: 52.45515552696097, Accuracy: 0.9674736495388669\n",
      "Accuracy with L2 regularization and lambda=1: 0.6961795872838817\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0, 0.001, 0.01, 0.1, 1]  # Different values of lambda for L2 regularization\n",
    "batch_size = 24 # Mini-batch size\n",
    "units = 32 # Number of units in the hidden layers\n",
    "for lambda_reg in lambdas:\n",
    "    print(f\"Training with lambda = {lambda_reg} and units = {units}\")\n",
    "    mlp = MLP(\n",
    "        LinearLayer(x_train.shape[-1], units),\n",
    "        ReLULayer(),\n",
    "        LinearLayer(units, units),\n",
    "        ReLULayer(),\n",
    "        LinearLayer(units, y_train.shape[-1]),\n",
    "        SoftmaxOutputLayer()\n",
    "    )\n",
    "    optimizer = RegGradientDescentOptimizer(mlp, lr=0.001, lambda_reg=lambda_reg)\n",
    "    history = mlp.fit(x_train, y_train, optimizer, iterations=10, batch_size=batch_size, lambda_reg=lambda_reg)\n",
    "    y_pred = mlp.predict(x_test)\n",
    "    accuracy = mlp.evaluate_acc(y_test, y_pred)\n",
    "    print(f\"Accuracy with L2 regularization and lambda={lambda_reg}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hidden_units \u001b[38;5;129;01min\u001b[39;00m hidden_units_options:\n\u001b[0;32m--> 104\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     results[hidden_units] \u001b[38;5;241m=\u001b[39m test_acc\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_units\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hidden units, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 81\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(hidden_units)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     83\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 64\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n\u001b[0;32m---> 64\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out))\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ecole/Uni/COMP/551/Assignments/A3/COMP551/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv('archive/sign_mnist_train.csv')\n",
    "test_df = pd.read_csv('archive/sign_mnist_test.csv')\n",
    "\n",
    "# Extract labels and features\n",
    "y_train = train_df.pop('label').values\n",
    "x_train = train_df.values\n",
    "y_test = test_df.pop('label').values\n",
    "x_test = test_df.values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train).astype(np.float32)\n",
    "x_test = scaler.transform(x_test).astype(np.float32)\n",
    "\n",
    "# Reshape the features\n",
    "x_train = x_train.reshape((-1, 1, 28, 28))\n",
    "x_test = x_test.reshape((-1, 1, 28, 28))\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "x_test_tensor = torch.tensor(x_test)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the ConvNet model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, hidden_units)  # 3x3 is the spatial dimension of the output volume\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_units, 26)  # 26 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate_model(hidden_units):\n",
    "    model = ConvNet(hidden_units)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(10):  # 10 iterations\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.argmax(dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "\n",
    "    test_acc = correct / total\n",
    "    return test_acc\n",
    "\n",
    "# Running the experiments\n",
    "hidden_units_options = [32, 64, 128, 256]\n",
    "results = {}\n",
    "for hidden_units in hidden_units_options:\n",
    "    test_acc = train_and_evaluate_model(hidden_units)\n",
    "    results[hidden_units] = test_acc\n",
    "    print(f\"Training with {hidden_units} hidden units, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#Find the best performance\n",
    "best_units = max(results, key=results.get)\n",
    "print(f\"Best performance with {best_units} hidden units: {results[best_units]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (24,24) (24,1,28,24) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m adam_optimizer \u001b[38;5;241m=\u001b[39m AdamOptimizer(mlp, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the MLP model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madam_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the test set\u001b[39;00m\n\u001b[1;32m     28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "Cell \u001b[0;32mIn[15], line 99\u001b[0m, in \u001b[0;36mMLP.fit\u001b[0;34m(self, x, y, optimizer, iterations, batch_size, lambda_reg, verbose)\u001b[0m\n\u001b[1;32m     96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x_batch)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Loss computation \u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m data_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Regularization loss\u001b[39;00m\n\u001b[1;32m    101\u001b[0m reg_loss \u001b[38;5;241m=\u001b[39m lambda_reg \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(layer\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[15], line 217\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(output, y_batch)\u001b[0m\n\u001b[1;32m    215\u001b[0m output_clipped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(output, \u001b[38;5;241m1e-7\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Compute cross-entropy loss\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_clipped\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (24,24) (24,1,28,24) "
     ]
    }
   ],
   "source": [
    "# MAKE SURE TO RUN THE FIRST PREPROCESSING CELL RIGHT BEFORE RUNNING THIS CELL \n",
    "# TO MAKE SURE THE DATA IS IN THE CORRECT FORMAT\n",
    "# THE ORDER IS EXPERIMENT 4, PREPROCESSING, EXPERIMENT 5\n",
    "preprocessing()\n",
    "\n",
    "# Define the MLP architecture \n",
    "units = 256 # If you want to skip running the previous cell you can use 128 or 256\n",
    "input_size = x_train.shape[-1]  # Determine the input size from the training data\n",
    "output_size = y_train.shape[-1]  # Determine the output size from the training data\n",
    "mlp = MLP(\n",
    "    LinearLayer(input_size, units),  \n",
    "    ReLULayer(),            \n",
    "    LinearLayer(units, units),  \n",
    "    ReLULayer(),            \n",
    "    LinearLayer(units, units),  \n",
    "    ReLULayer(),            \n",
    "    LinearLayer(units, output_size),   \n",
    "    SoftmaxOutputLayer()    \n",
    ")\n",
    "\n",
    "# Initialize the Optimizer \n",
    "adam_optimizer = AdamOptimizer(mlp, lr=0.001)\n",
    "\n",
    "# Train the MLP model\n",
    "mlp.fit(x_train, y_train, optimizer=adam_optimizer, iterations=10, verbose=True)\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "y_pred = mlp.predict(x_test)\n",
    "accuracy = mlp.evaluate_acc(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
